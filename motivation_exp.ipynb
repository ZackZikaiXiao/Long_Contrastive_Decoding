{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c0f0bc-b4a7-4dc4-970b-8d947cf9118b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install \"deepspeed==0.14.0\" --upgrade\n",
    "%pip install modelscope\n",
    "%pip install flash-attn==2.5.8 --no-build-isolation\n",
    "# %DS_BUILD_CPU_ADAM=1  BUILD_UTILS=1  pip install deepspeed==0.14.3 -U\n",
    "# %DS_BUILD_CPU_ADAM=1  BUILD_UTILS=1  pip install deepspeed==0.14.0 -U\n",
    "# %conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "\n",
    "# %pip install jieba\n",
    "# %pip install prettytable\n",
    "# %pip install accelerate --upgrade\n",
    "# %pip uninstall -y transformers\n",
    "# %pip install git+https://github.com/huggingface/transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context\"\n",
    "data_path = base_path + \"/data\"\n",
    "model_path = base_path + \"/models\"\n",
    "\n",
    "# model_id = snapshot_download(\"LLM-Research/Meta-Llama-3.1-8B-Instruct\", cache_dir=model_path)# 移动模型文件到models目录\n",
    "\n",
    "# subprocess.run(['mv', base_path + '/models/LLM-Research/Meta-Llama-3-8B-Instruct', base_path + '/models/llama3-8B-8k'], check=True)\n",
    "\n",
    "subprocess.run(['modelscope', 'download', '--model=LLM-Research/Meta-Llama-3.1-8B-Instruct', '--local_dir', model_path + '/llama-3.1-8B-128k-Instruct'], check=True)\n",
    "# huggingface-cli download meta-llama/Meta-Llama-3.1-8B --include \"original/*\" --local-dir Meta-Llama-3.1-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "environment= \"databricks\" # databricks / terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0328c225-176c-4e29-8c1e-bd4d533083b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if environment == \"databricks\":\n",
    "    base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context_decoding\"\n",
    "    source_file = base_path + \"/motivation\" + '/kv-retrieval-3000_keys.jsonl'\n",
    "    save_dir = base_path + \"/motivation/data\"\n",
    "    \n",
    "elif environment == \"terminal\":\n",
    "    source_file = './benchmark/super_retrieval/kv-retrieval-3000_keys.jsonl'\n",
    "    save_dir = './motivation/data'\n",
    "\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "!python motivation/generate_dataset.py --source_file {source_file} --save_dir {save_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if environment == \"databricks\":\n",
    "    base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context_decoding\"\n",
    "    model_path = '/Volumes/main/default/default_volume/erikyzzhang/long_context/models/llama-3-8B-262k'\n",
    "    data_path = base_path + \"/motivation/data\"\n",
    "    \n",
    "elif environment == \"terminal\":\n",
    "    model_path = '/home/zikaixiao/zikaixiao/LongLoRA-main/models/llama-3-8B-262k'\n",
    "    data_path = './motivation/data'\n",
    "\n",
    "!python motivation/generate_prob.py --model_path {model_path} --data_path {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if environment == \"databricks\":\n",
    "    base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context_decoding\"\n",
    "    model_path = '/Volumes/main/default/default_volume/erikyzzhang/long_context/models/llama-3-8B-262k'\n",
    "    data_path = base_path + \"/motivation/data\"\n",
    "    \n",
    "elif environment == \"terminal\":\n",
    "    model_path = '/home/zikaixiao/zikaixiao/LongLoRA-main/models/llama-3-8B-262k'\n",
    "    data_path = './motivation/data_32k'\n",
    "\n",
    "!python motivation/generate_result.py --model_path {model_path} --data_path {data_path}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_db",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
